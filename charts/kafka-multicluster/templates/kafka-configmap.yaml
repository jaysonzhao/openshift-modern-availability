apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
data:
  log4j.properties: |
    log4j.rootLogger=INFO, stdout

    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n

    #log4j.appender.fileAppender=org.apache.log4j.FileAppender
    #log4j.appender.fileAppender.File=kafka-request.log
    #log4j.appender.fileAppender.layout=org.apache.log4j.PatternLayout
    #log4j.appender.fileAppender.layout.ConversionPattern= %-4r [%t] %-5p %c %x - %m%n


    # Turn on all our debugging info
    log4j.logger.kafka=INFO,stdout
    #log4j.logger.kafka.producer.async.DefaultEventHandler=DEBUG,stdout
    #log4j.logger.kafka.consumer.PartitionTopicInfo=TRACE,stdout
    #log4j.logger.kafka.request.logger=TRACE,fileAppender
    #log4j.additivity.kafka.request.logger=false
    #log4j.logger.kafka.network.Processor=TRACE,fileAppender
    #log4j.additivity.kafka.network.Processor=false
    #log4j.logger.org.I0Itec.zkclient.ZkClient=DEBUG
  server.properties: |
    # see kafka.server.KafkaConfig for additional details and defaults

    # the id of the broker
    broker.id.generation.enable=true
    #brokerid=0

    # hostname of broker. If not set, will pick up from the value returned
    # from getLocalHost.  If there are multiple interfaces getLocalHost
    # may not be what you want.
    # hostname=

    # the port the socket server runs on
    port=9092

    # the number of processor threads the socket server uses. Defaults to the number of cores on the machine
    num.threads=8

    # The number of threads handling network requests
    num.network.threads=16
    
    # The number of threads doing disk I/O
    num.io.threads=16
    num.replica.fetchers=16
    num.recovery.threads.per.data.dir=16

    # the directory in which to store log files
    log.dirs=/data1/logs,/data2/logs,/data3/logs

    # the send buffer used by the socket server 
    #socket.send.buffer.bytes=1048576

    # using bandwidth-delay approach (https://en.wikipedia.org/wiki/Bandwidth-delay_product): 
    # 250MBps (as measured by subctl benchmark throughtput) and 70ms -> 250 * 10^6 b/s * 7 *10^-3 s = 1750*10^3 =~ 1.5MB 
    socket.send.buffer.bytes={{ div (mul .Values.latency .Values.bandwidth) 1000 }}
    # the receive buffer used by the socket server
    #socket.receive.buffer=1048576
    socket.receive.buffer.bytes={{ div (mul .Values.latency .Values.bandwidth) 1000 }}


    # the maximum size of a log segment
    log.file.size=536870912

    # the interval between running cleanup on the logs
    log.cleanup.interval.mins=1

    # the minimum age of a log file to eligible for deletion
    log.retention.hours=168

    #the number of messages to accept without flushing the log to disk
    log.flush.interval=1

    # time based topic flush intervals in ms
    #topic.flush.intervals.ms=topic:1000

    # default time based flush interval in ms
    log.default.flush.interval.ms=2000

    # the interval (in ms) at which logs are checked to see if they need to be flushed to disk.
    log.default.flush.scheduler.interval.ms=1000 

    listeners=BROKER://:9091,CONTROLLER://:9092,CLIENT://:9093
    listener.security.protocol.map=BROKER:SSL,CONTROLLER:SSL,CLIENT:SSL
    control.plane.listener.name=CONTROLLER
    inter.broker.listener.name=BROKER
    listener.name.broker.ssl.client.auth=required
    listener.name.controller.ssl.client.auth=required
    broker.rack={{ .Values.cluster }}
    ssl.endpoint.identification.algorithm=HTTPS

    auto.create.topics.enable=true
    default.replication.factor=3
    num.partitions=9
    min.insync.replicas=2
    auto.leader.rebalance.enable=true

    offsets.topic.num.partitions=9
    offsets.topic.replication.factor=3
    transaction.state.log.num.partitions=9
    transaction.state.log.replication.factor=3
    transaction.state.log.min.isr=2

    replica.socket.receive.buffer.bytes={{ div (mul .Values.latency .Values.bandwidth) 1000 }}
    replica.socket.receive.buffer.bytes={{ div (mul .Values.latency .Values.bandwidth) 1000 }}

    replica.lag.time.max.ms=5000
    group.initial.rebalance.delay.ms=3000

    #KRaft section
    #process.roles=broker,controller

    #ssl.client.auth=required
    ssl.keystore.location=/certs/keystore.jks
    ssl.keystore.password=changeit
    ssl.keystore.type=JKS
    ssl.truststore.location=/certs/truststore.jks
    ssl.truststore.password=changeit
    ssl.truststore.type=JKS

    #set the following properties to use zookeeper
    # enable connecting to zookeeper
    enable.zookeeper=true

    # zk connection string
    # comma separated host:port pairs, each corresponding to a zk
    # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"
    {{ $replicas:= .Values.zookeeper.replicas -}}
    {{- $namespace := .Release.Namespace  -}}
    zookeeper.connect=
    {{- range until (int $replicas) -}}
    {{- if eq (sub $replicas 1) . -}}
    zookeeper-{{ . }}.zookeeper.{{ $namespace }}.svc.cluster.local:2181
    {{- else -}}
    zookeeper-{{ . }}.zookeeper.{{ $namespace }}.svc.cluster.local:2181,
    {{- end -}}
    {{- end }}
    zookeeper.ssl.client.enable=true
    zookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty
    
    # timeout in ms for connecting to zookeeper
    zk.connectiontimeout.ms=1000000

    zookeeper.ssl.keystore.location=/certs/keystore.jks
    zookeeper.ssl.keystore.password=changeit
    zookeeper.ssl.keystore.type=JKS
    zookeeper.ssl.truststore.location=/certs/truststore.jks
    zookeeper.ssl.truststore.password=changeit
    zookeeper.ssl.truststore.type=JKS

  kafka-0.8.2.yaml: |
    startDelaySeconds: 10
    hostPort: 127.0.0.1:9080
    ssl: false 
    lowercaseOutputName: true
    rules:
    - pattern : kafka.cluster<type=(.+), name=(.+), topic=(.+), partition=(.+)><>Value
      name: kafka_cluster_$1_$2
      labels:
        topic: "$3"
        partition: "$4"
    - pattern : kafka.log<type=Log, name=(.+), topic=(.+), partition=(.+)><>Value
      name: kafka_log_$1
      labels:
        topic: "$2"
        partition: "$3"
    - pattern : kafka.controller<type=(.+), name=(.+)><>(Count|Value)
      name: kafka_controller_$1_$2
    - pattern : kafka.network<type=(.+), name=(.+)><>Value
      name: kafka_network_$1_$2
    - pattern : kafka.network<type=(.+), name=(.+)PerSec, request=(.+)><>Count
      name: kafka_network_$1_$2_total
      labels:
        request: "$3"
    - pattern : kafka.network<type=(.+), name=(\w+), networkProcessor=(.+)><>Count
      name: kafka_network_$1_$2
      labels:
        request: "$3"
      type: COUNTER
    - pattern : kafka.network<type=(.+), name=(\w+), request=(\w+)><>Count
      name: kafka_network_$1_$2
      labels:
        request: "$3"
    - pattern : kafka.network<type=(.+), name=(\w+)><>Count
      name: kafka_network_$1_$2
    - pattern : kafka.server<type=(.+), name=(.+)PerSec\w*, topic=(.+)><>Count
      name: kafka_server_$1_$2_total
      labels:
        topic: "$3"
    - pattern : kafka.server<type=(.+), name=(.+)PerSec\w*><>Count
      name: kafka_server_$1_$2_total
      type: COUNTER

    - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>(Count|Value)
      name: kafka_server_$1_$2
      labels:
        clientId: "$3"
        topic: "$4"
        partition: "$5"
    - pattern : kafka.server<type=(.+), name=(.+), topic=(.+), partition=(.*)><>(Count|Value)
      name: kafka_server_$1_$2
      labels:
        topic: "$3"
        partition: "$4"
    - pattern : kafka.server<type=(.+), name=(.+), topic=(.+)><>(Count|Value)
      name: kafka_server_$1_$2
      labels:
        topic: "$3"
      type: COUNTER

    - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><>(Count|Value)
      name: kafka_server_$1_$2
      labels:
        clientId: "$3"
        broker: "$4:$5"
    - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+)><>(Count|Value)
      name: kafka_server_$1_$2
      labels:
        clientId: "$3"
    - pattern : kafka.server<type=(.+), name=(.+)><>(Count|Value)
      name: kafka_server_$1_$2

    - pattern : kafka.(\w+)<type=(.+), name=(.+)PerSec\w*><>Count
      name: kafka_$1_$2_$3_total
    - pattern : kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, topic=(.+)><>Count
      name: kafka_$1_$2_$3_total
      labels:
        topic: "$4"
      type: COUNTER
    - pattern : kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, topic=(.+), partition=(.+)><>Count
      name: kafka_$1_$2_$3_total
      labels:
        topic: "$4"
        partition: "$5"
      type: COUNTER
    - pattern : kafka.(\w+)<type=(.+), name=(.+)><>(Count|Value)
      name: kafka_$1_$2_$3_$4
      type: COUNTER
    - pattern : kafka.(\w+)<type=(.+), name=(.+), (\w+)=(.+)><>(Count|Value)
      name: kafka_$1_$2_$3_$6
      labels:
        "$4": "$5"
  logging.properties: |
    handlers=java.util.logging.ConsoleHandler
    java.util.logging.ConsoleHandler.level=ALL
    io.prometheus.jmx.level=ALL
    io.prometheus.jmx.shaded.io.prometheus.jmx.level=ALL  


