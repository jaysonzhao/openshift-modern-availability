apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
data:
  log4j.properties: |
    log4j.rootLogger=INFO, stdout

    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n

    #log4j.appender.fileAppender=org.apache.log4j.FileAppender
    #log4j.appender.fileAppender.File=kafka-request.log
    #log4j.appender.fileAppender.layout=org.apache.log4j.PatternLayout
    #log4j.appender.fileAppender.layout.ConversionPattern= %-4r [%t] %-5p %c %x - %m%n


    # Turn on all our debugging info
    log4j.logger.kafka=INFO,stdout
    #log4j.logger.kafka.producer.async.DefaultEventHandler=DEBUG,stdout
    #log4j.logger.kafka.consumer.PartitionTopicInfo=TRACE,stdout
    #log4j.logger.kafka.request.logger=TRACE,fileAppender
    #log4j.additivity.kafka.request.logger=false
    #log4j.logger.kafka.network.Processor=TRACE,fileAppender
    #log4j.additivity.kafka.network.Processor=false
    #log4j.logger.org.I0Itec.zkclient.ZkClient=DEBUG
  server.properties: |
    # see kafka.server.KafkaConfig for additional details and defaults

    # the id of the broker
    brokerid=0

    # hostname of broker. If not set, will pick up from the value returned
    # from getLocalHost.  If there are multiple interfaces getLocalHost
    # may not be what you want.
    # hostname=

    # the port the socket server runs on
    port=9092

    # the number of processor threads the socket server uses. Defaults to the number of cores on the machine
    num.threads=8

    # the directory in which to store log files
    log.dir=/tmp/kafka-logs

    # the send buffer used by the socket server 
    socket.send.buffer=1048576

    # the receive buffer used by the socket server
    socket.receive.buffer=1048576

    # the maximum size of a log segment
    log.file.size=536870912

    # the interval between running cleanup on the logs
    log.cleanup.interval.mins=1

    # the minimum age of a log file to eligible for deletion
    log.retention.hours=168

    #the number of messages to accept without flushing the log to disk
    log.flush.interval=1

    #set the following properties to use zookeeper

    # enable connecting to zookeeper
    enable.zookeeper=true

    # zk connection string
    # comma separated host:port pairs, each corresponding to a zk
    # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"
    zookeeper.connect=zookeeper-client.kafka.svc.clusterset.local:2181
    zookeeper.ssl.client.enable=true

    # timeout in ms for connecting to zookeeper
    zk.connectiontimeout.ms=1000000

    # time based topic flush intervals in ms
    #topic.flush.intervals.ms=topic:1000

    # default time based flush interval in ms
    log.default.flush.interval.ms=2000

    # the interval (in ms) at which logs are checked to see if they need to be flushed to disk.
    log.default.flush.scheduler.interval.ms=1000

    # topic partition count map
    # topic.partition.count.map=topic1:3, topic2:4  

    listeners=BROKER://:9091,CONTROLLER://:9092,CLIENT://:9093
    listener.security.protocol.map=BROKER:SSL,CONTROLLER:SSL,CLIENT:SSL
    control.plane.listener.name=CONTROLLER
    inter.broker.listener.name=BROKER
    broker.rack={{ .Values.cluster }} 


    default.replication.factor=3
    num.partitions=100


    ssl.keystore.location=/certs/keystore.jks
    ssl.keystore.password=changeit
    ssl.keystore.type=JKS
    ssl.truststore.location=/certs/truststore.jks
    ssl.truststore.password=changeit
    ssl.truststore.type=JKS
    zookeeper.ssl.keystore.location=/certs/keystore.jks
    zookeeper.ssl.keystore.password=changeit
    zookeeper.ssl.keystore.type=JKS
    zookeeper.ssl.truststore.location=/certs/truststore.jks
    zookeeper.ssl.truststore.password=changeit
    zookeeper.ssl.truststore.type=JKS

